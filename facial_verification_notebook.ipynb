{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "G71CnRoyDZ8o"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsBXLMheA9gO",
        "outputId": "c58425b7-fed8-4ed4-d3f5-a0bc2dee0ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'facial-recognition'...\n",
            "remote: Enumerating objects: 931, done.\u001b[K\n",
            "remote: Counting objects: 100% (931/931), done.\u001b[K\n",
            "remote: Compressing objects: 100% (930/930), done.\u001b[K\n",
            "remote: Total 931 (delta 1), reused 930 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (931/931), 13.59 MiB | 31.41 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mirmirmirr/facial-recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### setup"
      ],
      "metadata": {
        "id": "3Q5XA_UkCuSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dependencies\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import numpy as mp\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "jxz_rgbXBj6U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorflow dependencies (model compenents and deep learning components)\n",
        "# using tensorflow funcational api\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n"
      ],
      "metadata": {
        "id": "YutY2FrtCgY-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# avoiding out of memory (OOM) errors by setting GPU Memory Consumption Growth\n",
        "gpu_list = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpu_list:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "metadata": {
        "id": "lnPRMZ-MC-YC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup paths\n",
        "POS_PATH = os.path.join('/content', 'facial-recognition', 'data', 'positive')\n",
        "NEG_PATH = os.path.join('/content', 'facial-recognition', 'data', 'neg')\n",
        "ANC_PATH = os.path.join('/content', 'facial-recognition', 'data', 'anchor')"
      ],
      "metadata": {
        "id": "vV2gCpBbDN05"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "POS_PATH+'\\*.jpg'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xBUulwV-ajWD",
        "outputId": "f1b712d8-a618-4600-873a-e5e8e0d892a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/facial-recognition/data/positive\\\\*.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### collecting postives and anchors"
      ],
      "metadata": {
        "id": "G71CnRoyDZ8o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VduEyFOmDfG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### preprocessing"
      ],
      "metadata": {
        "id": "6XaHPc7IDmx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting image directories\n",
        "\n",
        "## creating data sets from the respective data folders randomly\n",
        "postive = tf.data.Dataset.list_files(POS_PATH+'/*.jpg').take(300)\n",
        "negative = tf.data.Dataset.list_files(NEG_PATH+'/*.jpg').take(300)\n",
        "anchor = tf.data.Dataset.list_files(ANC_PATH+'/*.jpg').take(300)\n",
        "\n",
        "\n",
        "'''\n",
        "    accessing the dataset iterator: dir_txt = anchor.as_numpy_iterator()\n",
        "    going through each file in the dataset: dir_test.next()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Yh638T6ODq8J",
        "outputId": "652ec577-a646-466e-c39d-70416e486c60"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    accessing the dataset iterator: dir_txt = anchor.as_numpy_iterator() \\n    going through each file in the dataset: dir_test.next()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# image preprocessing - scaling and resizing\n",
        "\n",
        "'''\n",
        "ultimate goal/usage of preprocess funtion: dataset.map(preprocess)\n",
        "    this will allow the program to go through each image in the datasets created from the data folders\n",
        "\n",
        "returns the numpy equivalent of the image\n",
        "'''\n",
        "def preprocess(filePath):\n",
        "    image = tf.io.read_file(filePath)\n",
        "    img = tf.io.decode_jpeg(image)\n",
        "\n",
        "    # preprocessing -resizing to be 100x100x3 (pixels, pixels, channels)\n",
        "    img = tf.image.resize(img, (100, 100))\n",
        "\n",
        "    # scaling image to be between 0 and 1\n",
        "    ## traditonally, alll pixel values are between 0 and 255\n",
        "    img = img/255.0\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "4sTsqALlDvyT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating labeled dataset\n",
        "'''\n",
        "(anchor, postive) => 1,1,1,1,1      creating a \"right\" dataset\n",
        "(anchor, negative) => 0,0,0,0,0     creating a \"wrong\" dataset\n",
        "\n",
        "data form:\n",
        "<_ConcatenateDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>\n",
        "\n",
        "    shape:  ((), (), ())\n",
        "    type:   (tf.string, tf.string, tf.float32)\n",
        "            (anc filepath, pos/neg filepath, verification of 1 or 0 respectively)\n",
        "\n",
        "'''\n",
        "\n",
        "pos = tf.data.Dataset.zip((anchor, postive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
        "neg = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
        "data = pos.concatenate(neg)"
      ],
      "metadata": {
        "id": "7A_qWzRFD3xj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "usage:\n",
        "ex = data.as_numpy_iterator()\n",
        "ample = ex.next()\n",
        "preprocess_twin(*ample)\n",
        "'''\n",
        "\n",
        "def preprocess_twin(input, validation, label):\n",
        "    return (preprocess(input), preprocess(validation), label)"
      ],
      "metadata": {
        "id": "mAztecxzEEb0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building data pipeline\n",
        "data = data.map(preprocess_twin)\n",
        "data = data.cache()\n",
        "data = data.shuffle(buffer_size=1024)"
      ],
      "metadata": {
        "id": "re6KqPkzEKQk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training partition\n",
        "trained = data.take(round(len(data)*.7))\n",
        "trained = trained.batch(16)\n",
        "trained = trained.prefetch(8)"
      ],
      "metadata": {
        "id": "hnZo7Rl2EVQF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing partition\n",
        "tested = data.skip(round(len(data)*.7))     # skipping the first 70% of images so we don't take any form trained sample\n",
        "tested = tested.take(round(len(data)*.3))   # taking the last 30% of images for the testing sample\n",
        "tested = tested.batch(16)\n",
        "tested = tested.prefetch(8)"
      ],
      "metadata": {
        "id": "1fmzEkZBEYpb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model"
      ],
      "metadata": {
        "id": "K8vJuLLaEaYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding():\n",
        "    input = Input(shape=(100, 100, 3), name='input_image')\n",
        "\n",
        "    # core block 1\n",
        "    conv1 = Conv2D(64, (10, 10), activation='relu')(input)\n",
        "    mp1 = MaxPooling2D(64, (2,2), padding='same')(conv1)\n",
        "\n",
        "    # core block 2\n",
        "    conv2 = Conv2D(128, (7, 7), activation='relu')(mp1)\n",
        "    mp2 = MaxPooling2D(64, (2,2), padding='same')(conv2)\n",
        "\n",
        "    # core block 3\n",
        "    conv3 = Conv2D(128, (4, 4), activation='relu')(mp2)\n",
        "    mp3 = MaxPooling2D(64, (2,2), padding='same')(conv3)\n",
        "\n",
        "    # core block 4\n",
        "    conv4 = Conv2D(256, (4, 4), activation='relu')(mp3)\n",
        "    f1 = Flatten()(conv4)\n",
        "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
        "\n",
        "    return Model(inputs=[input], outputs=[d1], name='embedding' )\n"
      ],
      "metadata": {
        "id": "O60x46RmEcRm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = embedding()"
      ],
      "metadata": {
        "id": "KILHgpJtEkb1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHuNOeKmqTNc",
        "outputId": "da0932c7-8ea8-425e-ed69-47b5ab06a12f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"embedding\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_image (InputLayer)    [(None, 100, 100, 3)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 91, 91, 64)        19264     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 46, 46, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 40, 40, 128)       401536    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 20, 20, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 17, 17, 128)       262272    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 9, 9, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 6, 6, 256)         524544    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 9216)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              37752832  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 38,960,448\n",
            "Trainable params: 38,960,448\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom neural network layer\n",
        "\n",
        "# Siamese L1 Distance class\n",
        "class L1Dist(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "    def call(self, input_embedding, validation_embedding):\n",
        "        return tf.math.abs(input_embedding - validation_embedding)\n"
      ],
      "metadata": {
        "id": "a_sCYCclEnzY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model():\n",
        "\n",
        "    # inputs\n",
        "    input = Input(name='input_img', shape=(100, 100, 3))\n",
        "    validation = Input(name='validation_img', shape=(100, 100, 3))\n",
        "\n",
        "    # combine siamese distance components\n",
        "    siamese_layer = L1Dist()\n",
        "    siamese_layer._name = 'distance'\n",
        "    distances = siamese_layer(model(input), model(validation))\n",
        "\n",
        "    # classification layer\n",
        "    classifier = Dense(1, activation='sigmoid')(distances)\n",
        "\n",
        "    return Model(inputs=[input, validation], outputs=classifier, name='SiameseCat')\n"
      ],
      "metadata": {
        "id": "sC7d54m0EvSf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model = make_model()"
      ],
      "metadata": {
        "id": "_CVCCXxCExBF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training"
      ],
      "metadata": {
        "id": "5xRaQ1ovE0Ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up loss and optimizer\n",
        "binary_cross_loss = tf.losses.BinaryCrossentropy()\n",
        "opt = tf.keras.optimizers.Adam(1e-4)\n"
      ],
      "metadata": {
        "id": "p2OeGnm_E2EB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint callbacks incase there are problems with training\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(opt = opt, siamese_model=siamese_model)"
      ],
      "metadata": {
        "id": "vM3nZh3uE-QL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build train step function\n",
        "@tf.function\n",
        "def train_step(batch):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # get anchor and pos/neg image\n",
        "        X = batch[:2]\n",
        "        # get label\n",
        "        y = batch[2]\n",
        "\n",
        "        # forward pass\n",
        "        yhat = siamese_model(X, training=True)\n",
        "        # calculate loss\n",
        "        loss = binary_cross_loss(y, yhat)\n",
        "\n",
        "    # calculate gradients\n",
        "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
        "\n",
        "    # calculate weights and apply to model\n",
        "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "HwdLI6xJFB-A"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "def train(data, EPOCHS):\n",
        "\n",
        "    # loop through epochs\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
        "        progbar = tf.keras.utils.Progbar(len(data))\n",
        "\n",
        "        # oop through batches\n",
        "        for idx, batch in enumerate(data):\n",
        "            train_step(batch)\n",
        "            progbar.update(idx+1)\n",
        "\n",
        "        # save checkpoints\n",
        "        if epoch % 10 == 0:\n",
        "            checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "metadata": {
        "id": "wt8wIizYFFGW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50"
      ],
      "metadata": {
        "id": "EEVZ3fS6FHPy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(trained, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56ZL-P-7FIy0",
        "outputId": "3a247156-e4ec-43a4-ca7b-e2c4bd8a0b9b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1/50\n",
            "27/27 [==============================] - 18s 196ms/step\n",
            "\n",
            " Epoch 2/50\n",
            "27/27 [==============================] - 5s 178ms/step\n",
            "\n",
            " Epoch 3/50\n",
            "27/27 [==============================] - 5s 178ms/step\n",
            "\n",
            " Epoch 4/50\n",
            "27/27 [==============================] - 5s 177ms/step\n",
            "\n",
            " Epoch 5/50\n",
            "27/27 [==============================] - 5s 179ms/step\n",
            "\n",
            " Epoch 6/50\n",
            "27/27 [==============================] - 5s 180ms/step\n",
            "\n",
            " Epoch 7/50\n",
            "27/27 [==============================] - 5s 179ms/step\n",
            "\n",
            " Epoch 8/50\n",
            "27/27 [==============================] - 5s 180ms/step\n",
            "\n",
            " Epoch 9/50\n",
            "27/27 [==============================] - 5s 179ms/step\n",
            "\n",
            " Epoch 10/50\n",
            "27/27 [==============================] - 5s 180ms/step\n",
            "\n",
            " Epoch 11/50\n",
            "27/27 [==============================] - 5s 180ms/step\n",
            "\n",
            " Epoch 12/50\n",
            "27/27 [==============================] - 5s 175ms/step\n",
            "\n",
            " Epoch 13/50\n",
            "27/27 [==============================] - 5s 172ms/step\n",
            "\n",
            " Epoch 14/50\n",
            "27/27 [==============================] - 5s 173ms/step\n",
            "\n",
            " Epoch 15/50\n",
            "27/27 [==============================] - 5s 174ms/step\n",
            "\n",
            " Epoch 16/50\n",
            "27/27 [==============================] - 5s 175ms/step\n",
            "\n",
            " Epoch 17/50\n",
            "27/27 [==============================] - 5s 176ms/step\n",
            "\n",
            " Epoch 18/50\n",
            "27/27 [==============================] - 5s 177ms/step\n",
            "\n",
            " Epoch 19/50\n",
            "27/27 [==============================] - 5s 178ms/step\n",
            "\n",
            " Epoch 20/50\n",
            "27/27 [==============================] - 5s 177ms/step\n",
            "\n",
            " Epoch 21/50\n",
            "27/27 [==============================] - 5s 182ms/step\n",
            "\n",
            " Epoch 22/50\n",
            "27/27 [==============================] - 5s 177ms/step\n",
            "\n",
            " Epoch 23/50\n",
            "27/27 [==============================] - 5s 178ms/step\n",
            "\n",
            " Epoch 24/50\n",
            "27/27 [==============================] - 5s 175ms/step\n",
            "\n",
            " Epoch 25/50\n",
            "27/27 [==============================] - 5s 176ms/step\n",
            "\n",
            " Epoch 26/50\n",
            "27/27 [==============================] - 5s 176ms/step\n",
            "\n",
            " Epoch 27/50\n",
            "27/27 [==============================] - 5s 175ms/step\n",
            "\n",
            " Epoch 28/50\n",
            "27/27 [==============================] - 5s 177ms/step\n",
            "\n",
            " Epoch 29/50\n",
            "27/27 [==============================] - 5s 178ms/step\n",
            "\n",
            " Epoch 30/50\n",
            "27/27 [==============================] - 5s 178ms/step\n",
            "\n",
            " Epoch 31/50\n",
            "27/27 [==============================] - 5s 178ms/step\n",
            "\n",
            " Epoch 32/50\n",
            "27/27 [==============================] - 5s 178ms/step\n",
            "\n",
            " Epoch 33/50\n",
            "27/27 [==============================] - 5s 179ms/step\n",
            "\n",
            " Epoch 34/50\n",
            "27/27 [==============================] - 5s 180ms/step\n",
            "\n",
            " Epoch 35/50\n",
            "27/27 [==============================] - 5s 180ms/step\n",
            "\n",
            " Epoch 36/50\n",
            "27/27 [==============================] - 5s 180ms/step\n",
            "\n",
            " Epoch 37/50\n",
            "27/27 [==============================] - 5s 179ms/step\n",
            "\n",
            " Epoch 38/50\n",
            "27/27 [==============================] - 5s 180ms/step\n",
            "\n",
            " Epoch 39/50\n",
            "27/27 [==============================] - 5s 180ms/step\n",
            "\n",
            " Epoch 40/50\n",
            "27/27 [==============================] - 5s 180ms/step\n",
            "\n",
            " Epoch 41/50\n",
            "27/27 [==============================] - 5s 179ms/step\n",
            "\n",
            " Epoch 42/50\n",
            "27/27 [==============================] - 5s 180ms/step\n",
            "\n",
            " Epoch 43/50\n",
            "27/27 [==============================] - 5s 180ms/step\n",
            "\n",
            " Epoch 44/50\n",
            "27/27 [==============================] - 5s 181ms/step\n",
            "\n",
            " Epoch 45/50\n",
            "27/27 [==============================] - 5s 180ms/step\n",
            "\n",
            " Epoch 46/50\n",
            "27/27 [==============================] - 5s 181ms/step\n",
            "\n",
            " Epoch 47/50\n",
            "27/27 [==============================] - 5s 181ms/step\n",
            "\n",
            " Epoch 48/50\n",
            "27/27 [==============================] - 5s 181ms/step\n",
            "\n",
            " Epoch 49/50\n",
            "27/27 [==============================] - 5s 180ms/step\n",
            "\n",
            " Epoch 50/50\n",
            "27/27 [==============================] - 5s 181ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate model"
      ],
      "metadata": {
        "id": "YgUW5ibwL431"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import metric calculations\n",
        "from keras.metrics import Precision, Recall"
      ],
      "metadata": {
        "id": "hlg94MEzL9cp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get batch of test data\n",
        "test_input, test_val, y_true = tested.as_numpy_iterator().next()"
      ],
      "metadata": {
        "id": "ygpcLdQ6MMrS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions\n",
        "y_hat = siamese_model.predict([test_input, test_val])\n",
        "y_hat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ4llIL_NQFg",
        "outputId": "845ff262-5214-4d24-e4be-9dfcd9ebf28d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.9999309e-01],\n",
              "       [9.9999976e-01],\n",
              "       [9.9999905e-01],\n",
              "       [9.9999392e-01],\n",
              "       [9.9998355e-01],\n",
              "       [9.9998927e-01],\n",
              "       [5.0777514e-11],\n",
              "       [9.9996209e-01],\n",
              "       [9.9980706e-01],\n",
              "       [9.9999166e-01],\n",
              "       [1.0000000e+00],\n",
              "       [9.9999988e-01],\n",
              "       [1.0000000e+00],\n",
              "       [4.7239816e-12],\n",
              "       [1.5443680e-10],\n",
              "       [8.6299211e-11]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# post processing results\n",
        "\"\"\"\n",
        "result = []\n",
        "for prediction in y_hat:\n",
        "  if prediction > 0.5:\n",
        "    res.append(1)\n",
        "  else:\n",
        "    res.append(0)\n",
        "\"\"\"\n",
        "[1 if prediction > 0.5 else 0 for prediction in y_hat]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsxND49sNnXm",
        "outputId": "8e257eaa-f812-40c1-beb8-f3499fa79b85"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QLSz6BnOf-j",
        "outputId": "fd67a21e-78e1-4920-d23b-87fc68b841ab"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating metrics\n",
        "m = Recall()\n",
        "m.update_state(y_true, y_hat)\n",
        "m.result().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDvSkpILOkt1",
        "outputId": "87e73a31-c86f-4559-f4b3-4e59954879cd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = Precision()\n",
        "m.update_state(y_true, y_hat)\n",
        "m.result().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNOQwLJjO2QM",
        "outputId": "ac5d4d68-d5ff-4977-9a03-1b68647626f1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save model"
      ],
      "metadata": {
        "id": "maZjgCrUQpR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save weights\n",
        "siamese_model.save('siamesecat.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZTmA4Z0QssV",
        "outputId": "690354da-1c5c-4e86-ee84-8a4116c7fed4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reload model\n",
        "model = tf.keras.models.load_model('siamesecat.h5',\n",
        "                                   custom_objects = {'L1Dist': L1Dist, \"BinaryCossentropy\": tf.losses.BinaryCrossentropy})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7AcygwxQ8Wm",
        "outputId": "1760b91a-6cfb-4fb2-a93a-95080cb80f7f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict([test_input, test_val])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgBKRg37TKAi",
        "outputId": "dddc39fb-a0b4-4c86-b3a9-9c9b5a1fb61a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 207ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.9999309e-01],\n",
              "       [9.9999976e-01],\n",
              "       [9.9999905e-01],\n",
              "       [9.9999392e-01],\n",
              "       [9.9998355e-01],\n",
              "       [9.9998927e-01],\n",
              "       [5.0777514e-11],\n",
              "       [9.9996209e-01],\n",
              "       [9.9980706e-01],\n",
              "       [9.9999166e-01],\n",
              "       [1.0000000e+00],\n",
              "       [9.9999988e-01],\n",
              "       [1.0000000e+00],\n",
              "       [4.7239816e-12],\n",
              "       [1.5443680e-10],\n",
              "       [8.6299211e-11]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGVlxNLfTpaR",
        "outputId": "07980d7c-f727-49d3-fe5a-c3aa46ea4722"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"SiameseCat\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_img (InputLayer)         [(None, 100, 100, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " validation_img (InputLayer)    [(None, 100, 100, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " embedding (Functional)         (None, 4096)         38960448    ['input_img[0][0]',              \n",
            "                                                                  'validation_img[0][0]']         \n",
            "                                                                                                  \n",
            " l1_dist_1 (L1Dist)             (None, 4096)         0           ['embedding[0][0]',              \n",
            "                                                                  'embedding[1][0]']              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            4097        ['l1_dist_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 38,964,545\n",
            "Trainable params: 38,964,545\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GluLkgnKTyxM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}